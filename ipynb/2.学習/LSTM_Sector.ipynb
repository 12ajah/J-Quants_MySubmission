{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, pickle, os\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset as _Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from IPython.display import display, display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publish_datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00+09:00</th>\n",
       "      <td>-0.440823</td>\n",
       "      <td>0.191443</td>\n",
       "      <td>-0.008909</td>\n",
       "      <td>-0.319594</td>\n",
       "      <td>-0.072300</td>\n",
       "      <td>0.466741</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>0.445048</td>\n",
       "      <td>0.498942</td>\n",
       "      <td>0.032077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.343381</td>\n",
       "      <td>0.090035</td>\n",
       "      <td>0.199363</td>\n",
       "      <td>-0.338375</td>\n",
       "      <td>0.670792</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.632040</td>\n",
       "      <td>-0.433479</td>\n",
       "      <td>-0.011136</td>\n",
       "      <td>-0.051264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00+09:00</th>\n",
       "      <td>-0.351773</td>\n",
       "      <td>-0.027478</td>\n",
       "      <td>-0.060213</td>\n",
       "      <td>-0.512602</td>\n",
       "      <td>0.254371</td>\n",
       "      <td>0.128152</td>\n",
       "      <td>0.160517</td>\n",
       "      <td>0.154172</td>\n",
       "      <td>0.151672</td>\n",
       "      <td>0.255984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078464</td>\n",
       "      <td>0.179369</td>\n",
       "      <td>0.076657</td>\n",
       "      <td>-0.030442</td>\n",
       "      <td>-0.168868</td>\n",
       "      <td>-0.192623</td>\n",
       "      <td>0.602098</td>\n",
       "      <td>-0.076874</td>\n",
       "      <td>-0.616428</td>\n",
       "      <td>-0.406168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00+09:00</th>\n",
       "      <td>-0.115327</td>\n",
       "      <td>0.017725</td>\n",
       "      <td>-0.129011</td>\n",
       "      <td>-0.553259</td>\n",
       "      <td>0.096930</td>\n",
       "      <td>0.092610</td>\n",
       "      <td>0.150430</td>\n",
       "      <td>-0.043717</td>\n",
       "      <td>-0.105239</td>\n",
       "      <td>-0.155988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505299</td>\n",
       "      <td>0.315577</td>\n",
       "      <td>0.302115</td>\n",
       "      <td>0.032662</td>\n",
       "      <td>0.129836</td>\n",
       "      <td>-0.134002</td>\n",
       "      <td>0.500410</td>\n",
       "      <td>-0.120089</td>\n",
       "      <td>-0.640578</td>\n",
       "      <td>-0.581745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0         1         2         3         4    \\\n",
       "publish_datetime                                                              \n",
       "2020-01-01 00:00:00+09:00 -0.440823  0.191443 -0.008909 -0.319594 -0.072300   \n",
       "2020-01-01 00:00:00+09:00 -0.351773 -0.027478 -0.060213 -0.512602  0.254371   \n",
       "2020-01-01 00:00:00+09:00 -0.115327  0.017725 -0.129011 -0.553259  0.096930   \n",
       "\n",
       "                                5         6         7         8         9    \\\n",
       "publish_datetime                                                              \n",
       "2020-01-01 00:00:00+09:00  0.466741 -0.274743  0.445048  0.498942  0.032077   \n",
       "2020-01-01 00:00:00+09:00  0.128152  0.160517  0.154172  0.151672  0.255984   \n",
       "2020-01-01 00:00:00+09:00  0.092610  0.150430 -0.043717 -0.105239 -0.155988   \n",
       "\n",
       "                           ...       758       759       760       761  \\\n",
       "publish_datetime           ...                                           \n",
       "2020-01-01 00:00:00+09:00  ... -0.343381  0.090035  0.199363 -0.338375   \n",
       "2020-01-01 00:00:00+09:00  ... -0.078464  0.179369  0.076657 -0.030442   \n",
       "2020-01-01 00:00:00+09:00  ... -0.505299  0.315577  0.302115  0.032662   \n",
       "\n",
       "                                762       763       764       765       766  \\\n",
       "publish_datetime                                                              \n",
       "2020-01-01 00:00:00+09:00  0.670792  0.026400  0.632040 -0.433479 -0.011136   \n",
       "2020-01-01 00:00:00+09:00 -0.168868 -0.192623  0.602098 -0.076874 -0.616428   \n",
       "2020-01-01 00:00:00+09:00  0.129836 -0.134002  0.500410 -0.120089 -0.640578   \n",
       "\n",
       "                                767  \n",
       "publish_datetime                     \n",
       "2020-01-01 00:00:00+09:00 -0.051264  \n",
       "2020-01-01 00:00:00+09:00 -0.406168  \n",
       "2020-01-01 00:00:00+09:00 -0.581745  \n",
       "\n",
       "[3 rows x 768 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "headline_features = pd.read_pickle('../data/headline_features.pkl')\n",
    "new_headline_features = pd.read_pickle('../data/new_headline_features.pkl')\n",
    "new_headline_features.index = new_headline_features.index.tz_convert(headline_features.index.tz)\n",
    "\n",
    "# 結合\n",
    "headline_features = pd.concat([headline_features, new_headline_features], axis=0).copy()\n",
    "\n",
    "# 確認する。\n",
    "display(headline_features.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Local Code</th>\n",
       "      <th>EndOfDayQuote Date</th>\n",
       "      <th>EndOfDayQuote Open</th>\n",
       "      <th>EndOfDayQuote High</th>\n",
       "      <th>EndOfDayQuote Low</th>\n",
       "      <th>EndOfDayQuote Close</th>\n",
       "      <th>EndOfDayQuote ExchangeOfficialClose</th>\n",
       "      <th>EndOfDayQuote Volume</th>\n",
       "      <th>EndOfDayQuote CumulativeAdjustmentFactor</th>\n",
       "      <th>EndOfDayQuote PreviousClose</th>\n",
       "      <th>EndOfDayQuote PreviousCloseDate</th>\n",
       "      <th>EndOfDayQuote PreviousExchangeOfficialClose</th>\n",
       "      <th>EndOfDayQuote PreviousExchangeOfficialCloseDate</th>\n",
       "      <th>EndOfDayQuote ChangeFromPreviousClose</th>\n",
       "      <th>EndOfDayQuote PercentChangeFromPreviousClose</th>\n",
       "      <th>EndOfDayQuote VWAP</th>\n",
       "      <th>17 Sector(Code)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1301</td>\n",
       "      <td>2016/01/04</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>2820.0</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2770.0</td>\n",
       "      <td>2015/12/30</td>\n",
       "      <td>2770.0</td>\n",
       "      <td>2015/12/30</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>-0.722</td>\n",
       "      <td>2778.250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1301</td>\n",
       "      <td>2016/01/05</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>2780.0</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>2016/01/04</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>2016/01/04</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.364</td>\n",
       "      <td>2761.990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1301</td>\n",
       "      <td>2016/01/06</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>2770.0</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>2016/01/05</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>2016/01/05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2758.867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1301</td>\n",
       "      <td>2016/01/07</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>2710.0</td>\n",
       "      <td>2710.0</td>\n",
       "      <td>2710.0</td>\n",
       "      <td>31400.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>2016/01/06</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>2016/01/06</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-1.812</td>\n",
       "      <td>2733.471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1301</td>\n",
       "      <td>2016/01/08</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>2690.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2710.0</td>\n",
       "      <td>2016/01/07</td>\n",
       "      <td>2710.0</td>\n",
       "      <td>2016/01/07</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>2709.122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Local Code EndOfDayQuote Date  EndOfDayQuote Open  EndOfDayQuote High  \\\n",
       "0        1301         2016/01/04              2800.0              2820.0   \n",
       "1        1301         2016/01/05              2750.0              2780.0   \n",
       "2        1301         2016/01/06              2760.0              2770.0   \n",
       "3        1301         2016/01/07              2740.0              2760.0   \n",
       "4        1301         2016/01/08              2700.0              2740.0   \n",
       "\n",
       "   EndOfDayQuote Low  EndOfDayQuote Close  \\\n",
       "0             2740.0               2750.0   \n",
       "1             2750.0               2760.0   \n",
       "2             2740.0               2760.0   \n",
       "3             2710.0               2710.0   \n",
       "4             2690.0               2700.0   \n",
       "\n",
       "   EndOfDayQuote ExchangeOfficialClose  EndOfDayQuote Volume  \\\n",
       "0                               2750.0               32000.0   \n",
       "1                               2760.0               20100.0   \n",
       "2                               2760.0               15000.0   \n",
       "3                               2710.0               31400.0   \n",
       "4                               2700.0               26200.0   \n",
       "\n",
       "   EndOfDayQuote CumulativeAdjustmentFactor  EndOfDayQuote PreviousClose  \\\n",
       "0                                       0.1                       2770.0   \n",
       "1                                       0.1                       2750.0   \n",
       "2                                       0.1                       2760.0   \n",
       "3                                       0.1                       2760.0   \n",
       "4                                       0.1                       2710.0   \n",
       "\n",
       "  EndOfDayQuote PreviousCloseDate  \\\n",
       "0                      2015/12/30   \n",
       "1                      2016/01/04   \n",
       "2                      2016/01/05   \n",
       "3                      2016/01/06   \n",
       "4                      2016/01/07   \n",
       "\n",
       "   EndOfDayQuote PreviousExchangeOfficialClose  \\\n",
       "0                                       2770.0   \n",
       "1                                       2750.0   \n",
       "2                                       2760.0   \n",
       "3                                       2760.0   \n",
       "4                                       2710.0   \n",
       "\n",
       "  EndOfDayQuote PreviousExchangeOfficialCloseDate  \\\n",
       "0                                      2015/12/30   \n",
       "1                                      2016/01/04   \n",
       "2                                      2016/01/05   \n",
       "3                                      2016/01/06   \n",
       "4                                      2016/01/07   \n",
       "\n",
       "   EndOfDayQuote ChangeFromPreviousClose  \\\n",
       "0                                  -20.0   \n",
       "1                                   10.0   \n",
       "2                                    0.0   \n",
       "3                                  -50.0   \n",
       "4                                  -10.0   \n",
       "\n",
       "   EndOfDayQuote PercentChangeFromPreviousClose  EndOfDayQuote VWAP  \\\n",
       "0                                        -0.722            2778.250   \n",
       "1                                         0.364            2761.990   \n",
       "2                                         0.000            2758.867   \n",
       "3                                        -1.812            2733.471   \n",
       "4                                        -0.369            2709.122   \n",
       "\n",
       "   17 Sector(Code)  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stock_priceとstock_listをロードします。\n",
    "stock_price = pd.read_csv('../data/stock_price.csv.gz')\n",
    "stock_list = pd.read_csv('../data/stock_list.csv.gz')\n",
    "new_stock_price = pd.read_csv('../data/new_stock_price_all.csv', index_col=[0])\n",
    "\n",
    "# 結合する\n",
    "stock_price = pd.concat([stock_price, new_stock_price], axis=0).copy()\n",
    "stock_price.sort_values(by=['Local Code', 'EndOfDayQuote Date'], inplace=True)\n",
    "stock_price = pd.merge(stock_price, stock_list[['Local Code', '17 Sector(Code)']], on='Local Code') # セクター番号を追加\n",
    "stock_price.reset_index(drop=True, inplace=True)\n",
    "display(stock_price.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stock_listから投資対象銘柄を取得し、stock_priceの銘柄を絞り込む\n",
    "codes = stock_list[stock_list[\"universe_comp2\"] == True][\"Local Code\"].values\n",
    "stock_price = stock_price.loc[stock_price.loc[:, \"Local Code\"].isin(codes)]\n",
    "stock_price = stock_price[['EndOfDayQuote Date', 'Local Code', \"EndOfDayQuote Open\", \n",
    "                           \"EndOfDayQuote ExchangeOfficialClose\", \"17 Sector(Code)\"]]\n",
    "\n",
    "# それぞれのcolumn名をわかりやすく変更する\n",
    "stock_price = stock_price.rename(columns={\n",
    "    'EndOfDayQuote Date': 'date',\n",
    "    'Local Code': 'asset',\n",
    "    'EndOfDayQuote Open': 'open',\n",
    "    'EndOfDayQuote ExchangeOfficialClose': 'close',\n",
    "    '17 Sector(Code)': 'sector',\n",
    "})\n",
    "\n",
    "\n",
    "# データごとにindex形式が異なると大変扱いにくい。下記のコードより特徴量と同様のindexの形式を変更する。\n",
    "# pd.to_datetimeより、string形式の日付をpd.Timestamp形式に変換する\n",
    "# pd.Timestamp形式をpd.DatetimeIndex形式に変更し、time zoneをheadline_featuresと同様に設定する。\n",
    "# この際、headline_featuresとkeywords_featuresはarticlesのindexを使用しているため、timezoneが一致している。どちらを用いても良い。\n",
    "stock_price['date'] = pd.to_datetime(stock_price['date'])\n",
    "stock_price['date'] = pd.DatetimeIndex(stock_price['date']).tz_localize(headline_features.index.tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">open</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asset</th>\n",
       "      <th>1301</th>\n",
       "      <th>1332</th>\n",
       "      <th>1333</th>\n",
       "      <th>1375</th>\n",
       "      <th>1377</th>\n",
       "      <th>1379</th>\n",
       "      <th>1407</th>\n",
       "      <th>1413</th>\n",
       "      <th>1414</th>\n",
       "      <th>1417</th>\n",
       "      <th>...</th>\n",
       "      <th>9962</th>\n",
       "      <th>9974</th>\n",
       "      <th>9979</th>\n",
       "      <th>9983</th>\n",
       "      <th>9984</th>\n",
       "      <th>9987</th>\n",
       "      <th>9989</th>\n",
       "      <th>9991</th>\n",
       "      <th>9994</th>\n",
       "      <th>9997</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-06 00:00:00+09:00</th>\n",
       "      <td>2860.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>2770.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3635.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>1387.7</td>\n",
       "      <td>2173.0</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2704.0</td>\n",
       "      <td>5490.0</td>\n",
       "      <td>1663.0</td>\n",
       "      <td>63050.0</td>\n",
       "      <td>4569.0</td>\n",
       "      <td>4335.0</td>\n",
       "      <td>3915.0</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>2271.0</td>\n",
       "      <td>703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07 00:00:00+09:00</th>\n",
       "      <td>2864.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>2725.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3715.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1403.1</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>4610.0</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2765.0</td>\n",
       "      <td>5710.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>63250.0</td>\n",
       "      <td>4646.0</td>\n",
       "      <td>4430.0</td>\n",
       "      <td>3995.0</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>2296.0</td>\n",
       "      <td>706.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08 00:00:00+09:00</th>\n",
       "      <td>2892.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>2714.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3635.0</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>1413.1</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>4555.0</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2697.0</td>\n",
       "      <td>5790.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>62080.0</td>\n",
       "      <td>4583.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>3940.0</td>\n",
       "      <td>1111.0</td>\n",
       "      <td>2307.0</td>\n",
       "      <td>689.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3558 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             open                                              \\\n",
       "asset                        1301   1332    1333 1375    1377    1379    1407   \n",
       "date                                                                            \n",
       "2020-01-06 00:00:00+09:00  2860.0  639.0  2770.0  NaN  3635.0  1956.0  1387.7   \n",
       "2020-01-07 00:00:00+09:00  2864.0  634.0  2725.0  NaN  3715.0  1965.0  1403.1   \n",
       "2020-01-08 00:00:00+09:00  2892.0  624.0  2714.0  NaN  3635.0  1943.0  1413.1   \n",
       "\n",
       "                                                   ...   close          \\\n",
       "asset                        1413    1414    1417  ...    9962    9974   \n",
       "date                                               ...                   \n",
       "2020-01-06 00:00:00+09:00  2173.0  4485.0  1642.0  ...  2704.0  5490.0   \n",
       "2020-01-07 00:00:00+09:00  2130.0  4610.0  1658.0  ...  2765.0  5710.0   \n",
       "2020-01-08 00:00:00+09:00  2145.0  4555.0  1662.0  ...  2697.0  5790.0   \n",
       "\n",
       "                                                                            \\\n",
       "asset                        9979     9983    9984    9987    9989    9991   \n",
       "date                                                                         \n",
       "2020-01-06 00:00:00+09:00  1663.0  63050.0  4569.0  4335.0  3915.0  1098.0   \n",
       "2020-01-07 00:00:00+09:00  1685.0  63250.0  4646.0  4430.0  3995.0  1113.0   \n",
       "2020-01-08 00:00:00+09:00  1680.0  62080.0  4583.0  4400.0  3940.0  1111.0   \n",
       "\n",
       "                                          \n",
       "asset                        9994   9997  \n",
       "date                                      \n",
       "2020-01-06 00:00:00+09:00  2271.0  703.0  \n",
       "2020-01-07 00:00:00+09:00  2296.0  706.0  \n",
       "2020-01-08 00:00:00+09:00  2307.0  689.0  \n",
       "\n",
       "[3 rows x 3558 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">open</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asset</th>\n",
       "      <th>1301</th>\n",
       "      <th>1332</th>\n",
       "      <th>1333</th>\n",
       "      <th>1377</th>\n",
       "      <th>1379</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>2922</th>\n",
       "      <th>2923</th>\n",
       "      <th>2925</th>\n",
       "      <th>2929</th>\n",
       "      <th>2930</th>\n",
       "      <th>2931</th>\n",
       "      <th>4526</th>\n",
       "      <th>2296</th>\n",
       "      <th>1375</th>\n",
       "      <th>2932</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-06 00:00:00+09:00</th>\n",
       "      <td>2860.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>2770.0</td>\n",
       "      <td>3635.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>1896.0</td>\n",
       "      <td>6380.0</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1681.0</td>\n",
       "      <td>3665.0</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>763.0</td>\n",
       "      <td>2002.5</td>\n",
       "      <td>689.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07 00:00:00+09:00</th>\n",
       "      <td>2864.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>2725.0</td>\n",
       "      <td>3715.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>6360.0</td>\n",
       "      <td>3130.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>3660.0</td>\n",
       "      <td>2678.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>736.0</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2037.5</td>\n",
       "      <td>704.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08 00:00:00+09:00</th>\n",
       "      <td>2892.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>2714.0</td>\n",
       "      <td>3635.0</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>6350.0</td>\n",
       "      <td>3120.0</td>\n",
       "      <td>871.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1712.0</td>\n",
       "      <td>3620.0</td>\n",
       "      <td>2622.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>755.0</td>\n",
       "      <td>1992.5</td>\n",
       "      <td>695.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             open                                         \\\n",
       "asset                        1301   1332    1333    1377    1379    2001   \n",
       "date                                                                       \n",
       "2020-01-06 00:00:00+09:00  2860.0  639.0  2770.0  3635.0  1956.0  1660.0   \n",
       "2020-01-07 00:00:00+09:00  2864.0  634.0  2725.0  3715.0  1965.0  1665.0   \n",
       "2020-01-08 00:00:00+09:00  2892.0  624.0  2714.0  3635.0  1943.0  1660.0   \n",
       "\n",
       "                                                          ...   close          \\\n",
       "asset                        2002    2003    2004   2009  ...    2922    2923   \n",
       "date                                                      ...                   \n",
       "2020-01-06 00:00:00+09:00  1896.0  6380.0  3090.0  883.0  ...  1681.0  3665.0   \n",
       "2020-01-07 00:00:00+09:00  1866.0  6360.0  3130.0  865.0  ...  1725.0  3660.0   \n",
       "2020-01-08 00:00:00+09:00  1850.0  6350.0  3120.0  871.0  ...  1712.0  3620.0   \n",
       "\n",
       "                                                                            \\\n",
       "asset                        2925   2929   2930   2931    4526   2296 1375   \n",
       "date                                                                         \n",
       "2020-01-06 00:00:00+09:00  2615.0  473.0  708.0  763.0  2002.5  689.0  NaN   \n",
       "2020-01-07 00:00:00+09:00  2678.0  553.0  736.0  784.0  2037.5  704.0  NaN   \n",
       "2020-01-08 00:00:00+09:00  2622.0  563.0  729.0  755.0  1992.5  695.0  NaN   \n",
       "\n",
       "                                \n",
       "asset                     2932  \n",
       "date                            \n",
       "2020-01-06 00:00:00+09:00  NaN  \n",
       "2020-01-07 00:00:00+09:00  NaN  \n",
       "2020-01-08 00:00:00+09:00  NaN  \n",
       "\n",
       "[3 rows x 174 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_stock_price = {} # 0:all_sector, 1~17:sector\n",
    "all_universe = stock_price.drop('sector', axis=1).set_index(['date', 'asset']).sort_index()\n",
    "dict_stock_price[0] = all_universe.unstack()['2020-01-01':].copy()\n",
    "\n",
    "# indexを['sector', date', 'asset']順のpd.MultiIndex形式として設定する。\n",
    "stock_price = stock_price.set_index(['sector', 'date', 'asset']).sort_index()\n",
    "\n",
    "for i in range(1, 18):\n",
    "    # 各セクターの株価データを取り出し、ディクショナリに格納(2020年以降のデータであるので、2020年以前のデータを切り捨てる。)\n",
    "    dict_stock_price[i] = stock_price.loc[i, :].unstack()['2020-01-01':]\n",
    "    \n",
    "# 確認する\n",
    "display(dict_stock_price[0].head(3))\n",
    "display(dict_stock_price[1].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_weekly_group(df):\n",
    "    # index情報から、(year, week)の情報を得る。\n",
    "    return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
    "\n",
    "\n",
    "def build_weekly_features(features, boundary_week):\n",
    "    assert isinstance(boundary_week, tuple)\n",
    "\n",
    "    weekly_group = _build_weekly_group(df=features)\n",
    "    features = features.groupby(weekly_group).apply(lambda x: x[:])\n",
    "\n",
    "    train_features = features[features.index.get_level_values(0) <= boundary_week]\n",
    "    test_features = features[features.index.get_level_values(0) > boundary_week]\n",
    "\n",
    "    return {'train': train_features, 'test': test_features}\n",
    "\n",
    "def build_weekly_labels(stock_price, boundary_week):\n",
    "    def _compute_weekly_return(x):\n",
    "        # その週の初営業日のopenから最終営業日のcloseまでのリターンを計算する。\n",
    "        weekly_return = ((x['close'].iloc[-1] - x['open'].iloc[0]) / x['open'].iloc[0])\n",
    "\n",
    "        # その日のvolumneが0であるデータは、openが0となっている。\n",
    "        # openが0の場合、np.infの値となっているため、np.nanに変換し除去する。\n",
    "        # 銘柄ごとのリターンを単純平均し、marketのweekly_returnを計算する。\n",
    "        return weekly_return.replace([np.inf, -np.inf], np.nan).dropna().mean()\n",
    "\n",
    "    assert isinstance(boundary_week, tuple)\n",
    "\n",
    "    weekly_group = _build_weekly_group(df=stock_price)\n",
    "    weekly_fwd_return = stock_price.groupby(weekly_group).apply(_compute_weekly_return).shift(-1).dropna()\n",
    "\n",
    "    train_labels = weekly_fwd_return[weekly_fwd_return.index <= boundary_week]\n",
    "    test_labels = weekly_fwd_return[weekly_fwd_return.index > boundary_week]\n",
    "\n",
    "    train_labels = (train_labels >= 0) * 1.0\n",
    "    test_labels = (test_labels >= 0) * 1.0\n",
    "\n",
    "    return {'train': train_labels, 'test': test_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回、学習に用いる週次のデータセットにおいて、LSTMの学習には週毎のニュースの件数が若干不足している傾向にあることから、過学習防止のため、少し工夫をしています。具体的には、全体的な特徴量(ニュースの情報)の順序は維持しつつ複数に分割し、その分割の中でシャッフルを行う方法を取ります。この方法を用いることで、モデルに入力するデータを増やすことでき、過学習を防止に繋がる効果が期待できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上記のコードをまとめて、pytorchのDatasetクラスを作成する\n",
    "\n",
    "class Dataset(_Dataset):\n",
    "    def __init__(self, weekly_features, weekly_labels, max_sequence_length):\n",
    "        # 共通する週のみを使うため、共通するindex情報を取得する\n",
    "        mask_index = (\n",
    "            weekly_features.index.get_level_values(0).unique() & weekly_labels.index\n",
    "        )\n",
    "\n",
    "        # 共通するindexのみのデータだけでreindexを行う。\n",
    "        self.weekly_features = weekly_features[\n",
    "            weekly_features.index.get_level_values(0).isin(mask_index)\n",
    "        ]\n",
    "        self.weekly_labels = weekly_labels.reindex(mask_index)\n",
    "        \n",
    "        # idからweekの情報を取得できるよう、id_to_weekをビルドする\n",
    "        self.id_to_week = {\n",
    "            id: week for id, week in enumerate(sorted(weekly_labels.index))\n",
    "        }\n",
    "\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def _shuffle_by_local_split(self, x, split_size=50):\n",
    "        return torch.cat(\n",
    "            [\n",
    "                splitted[torch.randperm(splitted.size()[0])]\n",
    "                for splitted in x.split(split_size, dim=0)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.weekly_labels)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        # 付与されたidから週の情報を取得し、その週の情報から、特徴量とラベルを取得する。\n",
    "        week = self.id_to_week[id]\n",
    "        x = self.weekly_features.xs(week, axis=0, level=0)[-self.max_sequence_length :]\n",
    "        y = self.weekly_labels.loc[week]\n",
    "\n",
    "        # pytorchでは、データをtorch.Tensorタイプとして扱うことが要求される。\n",
    "        # 全体的な特徴量(ニュースの情報)の順序は維持しつつ、入力とする特徴量を数分割し、その分割の中でシャッフルを行う。\n",
    "        x = self._shuffle_by_local_split(torch.tensor(x.values, dtype=torch.float))\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "        # max_sequence_lengthに最大のsequenceを合わせ、sequenceがmax_sequence_lengthに達しない場合は、前から0を埋め、sequenceを合わせる\n",
    "        if x.size()[0] < self.max_sequence_length:\n",
    "            x = F.pad(x, pad=(0, 0, self.max_sequence_length - x.size()[0], 0))\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMによる特徴量合成モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCombiner(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_size=18, num_layers=2): # 768, 128\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTMの定義\n",
    "        # batch_firstより、出力次元の最初がbatchとなる。\n",
    "        # dropoutを用いて、内部状態のconnectionをdropすることより過学習を防ぐ。\n",
    "        # Sequenceがかなり長く、入力の始めの方の情報の消失を防ぐため、bidirectionalのモデルを使う。\n",
    "        self.cell = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # より高次元の特徴量を抽出できるようにするため、classifierの手前で、compress_dim次元への線形圧縮を行う。\n",
    "        self.compressor = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "        # sentiment probabilityの出力層。\n",
    "        self.classifier = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "        # outputの範囲を[0, 1]とする。\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 入力値xから出力までの流れを定義する。\n",
    "        output, _ = self.cell(x)\n",
    "        output = self.sigmoid(self.classifier(self.compressor(output[:, -1, :])))\n",
    "        return output\n",
    "\n",
    "    '''\n",
    "    def extract_feature(self, x):\n",
    "        # 入力値xから特徴量抽出までの流れを定義する。\n",
    "        output, _ = self.cell(x)\n",
    "        output = self.compressor(output[:, -1, :])\n",
    "        return output\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCombinerHandler:\n",
    "    def __init__(self, feature_combiner_params, store_dir):\n",
    "        # モデル学習及び推論に用いるデバイスを定義する\n",
    "        if torch.cuda.device_count() >= 1:\n",
    "            self.device = 'cuda'\n",
    "            print(\"[+] Set Device: GPU\")\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            print(\"[+] Set Device: CPU\")\n",
    "\n",
    "        # モデルのcheckpointや抽出した特徴量及びsentimentをstoreする場所を定義する。\n",
    "        self.store_dir = store_dir\n",
    "        os.makedirs(store_dir, exist_ok=True)\n",
    "\n",
    "        # 上記で作成したfeaturecombinerを定義する。\n",
    "        self.feature_combiner = FeatureCombiner(**feature_combiner_params).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # 学習に用いるoptimizerを定義する。\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            params=self.feature_combiner.parameters(), lr=0.001,\n",
    "        )\n",
    "\n",
    "        # ロス関数の定義\n",
    "        self.criterion = nn.BCELoss().to(self.device)\n",
    "\n",
    "        # モデルのcheck pointが存在する場合、モデルをロードする\n",
    "        self._load_model()\n",
    "\n",
    "    # 学習に必要なデータ(並列のためbatch化されたもの)をサンプルする。\n",
    "    def _sample_xy(self, data_type):\n",
    "        assert data_type in (\"train\", \"val\")\n",
    "\n",
    "        # data_typeより、data_typeに合致したデータを取得するようにしている。\n",
    "        if data_type == \"train\":\n",
    "            # dataloaderをiteratorとして定義し、next関数として毎時のデータをサンプルすることができる。\n",
    "            # Iteratorは全てのデータがサンプルされると、StopIterationのエラーを発するが、そのようなエラーが出たとき、\n",
    "            # Iteratorを再定義し、データをサンプルするようにしている。\n",
    "            try:\n",
    "                x, y = next(self.iterable_train_dataloader)\n",
    "            except StopIteration:\n",
    "                self.iterable_train_dataloader = iter(self.train_dataloader)\n",
    "                x, y = next(self.iterable_train_dataloader)\n",
    "\n",
    "        elif data_type == \"val\":\n",
    "            try:\n",
    "                x, y = next(self.iterable_val_dataloader)\n",
    "            except StopIteration:\n",
    "                self.iterable_val_dataloader = iter(self.val_dataloader)\n",
    "                x, y = next(self.iterable_val_dataloader)\n",
    "\n",
    "        return x.to(self.device), y.to(self.device)\n",
    "\n",
    "    # モデルのパラメータをアップデートするロジック\n",
    "    def _update_params(self, loss):\n",
    "        # ロスから、gradientを逆伝播し、パラメータをアップデートする\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # 学習されたfeature_combinerのパラメータをcheck_pointとしてstoreするロジック\n",
    "    def _save_model(self, epoch):\n",
    "        torch.save(\n",
    "            self.feature_combiner.state_dict(),\n",
    "            os.path.join(self.store_dir, f\"{epoch}.ckpt\"),\n",
    "        )\n",
    "        print(f\"[+] Epoch: {epoch}, Model is saved.\")\n",
    "\n",
    "    # 学習されたcheckpointが存在す場合、feature_combinerにそのパラメータをロードするロジック\n",
    "    def _load_model(self):\n",
    "        # cudaで学習されたモデルなどを、cpu環境下でロードするときはこのパラメータが必要となる。\n",
    "        params_to_load = {}\n",
    "        if self.device == \"cpu\":\n",
    "            params_to_load[\"map_location\"] = torch.device(\"cpu\")\n",
    "\n",
    "        # .ckptファイルを探し、古い順から新しい順にソートする。\n",
    "        check_points = glob(os.path.join(self.store_dir, \"*.ckpt\"))\n",
    "        check_points = sorted(\n",
    "            check_points, key=lambda x: int(x.split(\"/\")[-1].replace(\".ckpt\", \"\")),\n",
    "        )\n",
    "\n",
    "        # check_pointが存在しない場合は、スキップする。\n",
    "        if len(check_points) == 0:\n",
    "            print(\"[!] No exists checkpoint\")\n",
    "            return\n",
    "\n",
    "        # 複数個のchieck_pointが存在する場合、一番最新のものを使い、モデルのパラメータをロードする\n",
    "        check_point = check_points[-1]\n",
    "        self.feature_combiner.load_state_dict(torch.load(check_point, **params_to_load))\n",
    "        print(\"[+] Model is loaded\")\n",
    "\n",
    "    # Datasetからdataloaderを定義するロジック\n",
    "    def _build_dataloader(\n",
    "        self, dataloader_params, weekly_features, weekly_labels, max_sequence_length\n",
    "    ):\n",
    "        # 上記3で作成したしたdatasetを定義する\n",
    "        dataset = Dataset(\n",
    "            weekly_features=weekly_features,\n",
    "            weekly_labels=weekly_labels,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "        )\n",
    "\n",
    "        # datasetのdataをiterableにロードできるよう、dataloaderを定義する、このとき、shuffle=Trueを渡すことで、データはランダムにサンプルされるようになる。\n",
    "        return DataLoader(dataset=dataset, shuffle=True, **dataloader_params)\n",
    "\n",
    "    # train用に、featuresとlabelsを渡し、datasetを定義し、dataloaderを定義するロジック\n",
    "    def set_train_dataloader(\n",
    "        self, dataloader_params, weekly_features, weekly_labels, max_sequence_length\n",
    "    ):\n",
    "        self.train_dataloader = self._build_dataloader(\n",
    "            dataloader_params=dataloader_params,\n",
    "            weekly_features=weekly_features,\n",
    "            weekly_labels=weekly_labels,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "        )\n",
    "\n",
    "        # dataloaderからiteratorを定義する\n",
    "        # iteratorはnext関数よりデータをサンプルすることが可能となる。\n",
    "        self.iterable_train_dataloader = iter(self.train_dataloader)\n",
    "\n",
    "    # validation用に、featuresとlabelsを渡し、datasetを定義し、dataloaderを定義するロジック\n",
    "    def set_val_dataloader(\n",
    "        self, dataloader_params, weekly_features, weekly_labels, max_sequence_length\n",
    "    ):\n",
    "        self.val_dataloader = self._build_dataloader(\n",
    "            dataloader_params=dataloader_params,\n",
    "            weekly_features=weekly_features,\n",
    "            weekly_labels=weekly_labels,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "        )\n",
    "\n",
    "        # dataloaderからiteratorを定義する\n",
    "        # iteratorはnext関数よりデータをサンプルすることが可能となる。\n",
    "        self.iterable_val_dataloader = iter(self.val_dataloader)\n",
    "\n",
    "    # 学習ロジック\n",
    "    def train(self, n_epoch):\n",
    "        # n_epochの回数分、全学習データを複数回用いて学習する。\n",
    "        for epoch in range(n_epoch):\n",
    "\n",
    "            # 各々のepochごとのaverage lossを表示するため、lossをstoreするリストを定義する。\n",
    "            train_losses = []\n",
    "            test_losses = []\n",
    "\n",
    "            # train_dataloaderの長さは、全ての学習データを一度用いるときの長さと同様である。\n",
    "            # batchを組むと、その分train_dataloaderの長さは可変し、ちょうど一度全てのデータで学習できる長さを返す。\n",
    "            for iter_ in tqdm(range(len(self.train_dataloader))):\n",
    "                # パラメータをtrainableにするため、feature_combinerをtrainモードにする。\n",
    "                self.feature_combiner.train()\n",
    "\n",
    "                # trainデータをサンプルする。\n",
    "                x, y = self._sample_xy(data_type=\"train\")\n",
    "\n",
    "                # feature_combinerに特徴量を入力し、sentiment scoreを取得する。\n",
    "                preds = self.feature_combiner(x=x)\n",
    "\n",
    "                # sentiment scoreとラベルとのロスを計算する。\n",
    "                train_loss = self.criterion(preds, y)\n",
    "\n",
    "                # 計算されたロスは、後ほどepochごとのdisplayに使用するため、storeしておく。\n",
    "                train_losses.append(train_loss.detach().cpu())\n",
    "\n",
    "                # lossから、gradientを逆伝播させ、パラメータをupdateする。\n",
    "                self._update_params(loss=train_loss)\n",
    "\n",
    "                # validation用のロースを計算する。\n",
    "                # 毎回計算を行うとコストがかかってくるので、iter_毎5回ごとに計算を行う。\n",
    "                if iter_ % 5 == 0:\n",
    "\n",
    "                    # 学習を行わないため、feature_combinerをevalモードにしておく。\n",
    "                    # evalモードでは、dropoutの影響を受けない。\n",
    "                    self.feature_combiner.eval()\n",
    "\n",
    "                    # 各パラメータごとのgradientを計算するとリソースが高まる。\n",
    "                    # evaluationの時には、gradient情報を持たせないことで、メモリーの節約に繋がる。\n",
    "                    with torch.no_grad():\n",
    "                        # validationデータをサンプルする\n",
    "                        x, y = self._sample_xy(data_type=\"val\")\n",
    "\n",
    "                        # feature_combinerに特徴量を入力し、sentiment scoreを取得する。\n",
    "                        preds = self.feature_combiner(x=x)\n",
    "\n",
    "                        # sentiment scoreとラベルとのロスを計算する。\n",
    "                        test_loss = self.criterion(preds, y)\n",
    "\n",
    "                        # 計算されたロスは、後ほどepochごとのdisplayに使用するため、storeしておく。\n",
    "                        test_losses.append(test_loss.detach().cpu())\n",
    "\n",
    "            # 毎epoch終了後、平均のロスをプリントする。\n",
    "            print(\n",
    "                f\"epoch: {epoch}, train_loss: {np.mean(train_losses):.4f}, val_loss: {np.mean(test_losses):.4f}\"\n",
    "            )\n",
    "\n",
    "            # 毎epoch終了後、モデルのパラメータをstoreする。\n",
    "            self._save_model(epoch=epoch)\n",
    "\n",
    "    # 特徴量から、合成特徴量を抽出するロジック\n",
    "    def combine_features(self, features):\n",
    "        # 学習を行わないため、feature_combinerをevalモードにしておく。\n",
    "        self.feature_combiner.eval()\n",
    "\n",
    "        # gradient情報を持たせないことで、メモリーの節約する。\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # 特徴量をfeature_combinerのextract_feature関数に入力し、出力層手前の特徴量を抽出する。\n",
    "            # 抽出するとき、tensorをcpu上に落とし、np.ndarray形式に変換する。\n",
    "            return (\n",
    "                self.feature_combiner.extract_feature(\n",
    "                    x=torch.tensor(features, dtype=torch.float).to(self.device)\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "    # 特徴量から、翌週のsentimentを予測するロジック\n",
    "    def predict_sentiment(self, features):\n",
    "        # 学習を行わないため、feature_combinerをevalモードにしておく。\n",
    "        self.feature_combiner.eval()\n",
    "\n",
    "        # gradient情報を持たせないことで、メモリーの節約する。\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # 特徴量をfeature_combinerに入力し、sentiment scoreを抽出する。\n",
    "            # 抽出するとき、tensorをcpu上に落とし、np.ndarray形式に変換する。\n",
    "            return (\n",
    "                self.feature_combiner(x=torch.tensor(features, dtype=torch.float).to(self.device))\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "    # weeklyグループされた特徴量を入力に、合成特徴量もしくは、sentiment scoreを抽出するロジック\n",
    "    def generate_by_weekly_features(\n",
    "        self, weekly_features, generate_target, max_sequence_length\n",
    "    ):\n",
    "        assert generate_target in (\"features\", \"sentiment\")\n",
    "        generate_func = getattr(\n",
    "            self,\n",
    "            {\"features\": \"combine_features\", \"sentiment\": \"predict_sentiment\"}[\n",
    "                generate_target\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # グループごとに特徴量もしくは、sentiment scoreを抽出し、最終的に重ねて返すため、リストを作成する。\n",
    "        outputs = []\n",
    "\n",
    "        # ユニークな週indexを取得する。\n",
    "        weeks = sorted(weekly_features.index.get_level_values(0).unique())\n",
    "\n",
    "        for week in tqdm(weeks):\n",
    "            # 各週ごとの特徴量を取得し、直近から、max_sequence_length分切る。\n",
    "            features = weekly_features.xs(week, axis=0, level=0)[-max_sequence_length:]\n",
    "\n",
    "            # 特徴量をモデルに入力し、合成特徴量もしくは、sentiment scoreを抽出し、outputsにappendする。\n",
    "            # np.expand_dims(features, axis=0)を用いる理由は、特徴量合成機の入力期待値は、dimention0がbatchであるが、\n",
    "            # featuresは、[1000, 768]の次元をもち、これらをunsqueezeし、[1, 1000, 768]に変換する必要がある。\n",
    "            outputs.append(generate_func(features=np.expand_dims(features, axis=0)))\n",
    "\n",
    "        # outputsを重ね、indexの情報とともにpd.DataFrame形式として返す。\n",
    "        return pd.DataFrame(np.concatenate(outputs, axis=0), index=weeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量合成モデルの学習及び特徴量合成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n"
     ]
    }
   ],
   "source": [
    "boundary_week = (2020, 53)\n",
    "features = headline_features\n",
    "weekly_features = build_weekly_features(features, boundary_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n",
      "<ipython-input-6-94966c175c5c>:3: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return pd.Series(list(zip(df.index.year, df.index.week)), index=df.index)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2020</th>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4    5    6    7    8    9    10   11   12   13  \\\n",
       "2020 2  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "     3  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "     4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "\n",
       "         14   15   16   17  \n",
       "2020 2  0.0  0.0  0.0  0.0  \n",
       "     3  0.0  0.0  1.0  1.0  \n",
       "     4  0.0  1.0  1.0  0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2021</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4    5    6    7    8    9    10   11   12   13  \\\n",
       "2021 1  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0   \n",
       "     2  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0   \n",
       "     3  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "         14   15   16   17  \n",
       "2021 1  0.0  0.0  0.0  0.0  \n",
       "     2  1.0  0.0  0.0  1.0  \n",
       "     3  0.0  0.0  0.0  0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 各セクターのラベルを結合する\n",
    "weekly_labels = build_weekly_labels(dict_stock_price[0], boundary_week)\n",
    "weekly_labels_train = weekly_labels['train'].copy()\n",
    "weekly_labels_test = weekly_labels['test'].copy()\n",
    "\n",
    "for i in range(1, 18):\n",
    "    weekly_labels = build_weekly_labels(dict_stock_price[i], boundary_week)\n",
    "    weekly_labels_train = pd.concat([weekly_labels_train, weekly_labels['train']], axis=1)\n",
    "    weekly_labels_test = pd.concat([weekly_labels_test, weekly_labels['test']], axis=1)\n",
    "\n",
    "weekly_labels_train.columns = [i for i in range(18)]\n",
    "weekly_labels_test.columns = [i for i in range(18)]\n",
    "weekly_labels_train.index = pd.MultiIndex.from_tuples(weekly_labels_train.index)\n",
    "weekly_labels_test.index = pd.MultiIndex.from_tuples(weekly_labels_test.index)\n",
    "\n",
    "display(weekly_labels_train.head(3))\n",
    "display(weekly_labels_test.head(3))\n",
    "\n",
    "weekly_labels.clear()\n",
    "weekly_labels['train'] = weekly_labels_train\n",
    "weekly_labels['test'] = weekly_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Set Device: CPU\n",
      "[!] No exists checkpoint\n"
     ]
    }
   ],
   "source": [
    "feature_combiner_handler = FeatureCombinerHandler(feature_combiner_params={\"input_size\": 768, \"hidden_size\": 128}, store_dir='./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataloaderをsetする。\n",
    "feature_combiner_handler.set_train_dataloader(\n",
    "    dataloader_params={\n",
    "        \"batch_size\": 4,\n",
    "        \"num_workers\": 2,\n",
    "    },\n",
    "    weekly_features=weekly_features['train'],\n",
    "    weekly_labels=weekly_labels['train'],\n",
    "    max_sequence_length=1000\n",
    ")\n",
    "\n",
    "# validation dataloaderをsetする。\n",
    "feature_combiner_handler.set_val_dataloader(\n",
    "    dataloader_params={\n",
    "        \"batch_size\": 4,\n",
    "        \"num_workers\": 2,\n",
    "    },\n",
    "    weekly_features=weekly_features['test'],\n",
    "    weekly_labels=weekly_labels['test'],\n",
    "    max_sequence_length=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4956, 0.5101, 0.4692, 0.4938, 0.5008, 0.5303, 0.4837, 0.5153, 0.4873,\n",
       "         0.5172, 0.5067, 0.5068, 0.4868, 0.5066, 0.4784, 0.5138, 0.5002, 0.4945],\n",
       "        [0.4981, 0.5100, 0.4752, 0.5052, 0.4970, 0.5311, 0.4868, 0.5112, 0.4910,\n",
       "         0.5061, 0.5029, 0.5191, 0.4871, 0.5036, 0.4877, 0.5238, 0.4999, 0.4979],\n",
       "        [0.5063, 0.5126, 0.4760, 0.5137, 0.5057, 0.5349, 0.4781, 0.5056, 0.4974,\n",
       "         0.5227, 0.4993, 0.5007, 0.4836, 0.5183, 0.4879, 0.5162, 0.5002, 0.4907],\n",
       "        [0.4961, 0.5051, 0.4834, 0.4965, 0.4965, 0.5317, 0.4825, 0.5090, 0.4943,\n",
       "         0.5255, 0.4979, 0.5203, 0.4861, 0.5030, 0.4796, 0.5100, 0.5063, 0.4970]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(49.9232)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "tmp = iter(feature_combiner_handler.train_dataloader)\n",
    "y = tmp.next()[1]\n",
    "pred = feature_combiner_handler.feature_combiner(tmp.next()[0])\n",
    "display(y)\n",
    "display(pred)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "loss(y.detach(), pred.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルチターゲットにおける損失関数について  \n",
    "https://towardsdatascience.com/multi-label-image-classification-with-neural-network-keras-ddc1ab1afede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae864ddca5e4e74a9b3255f6e5f90fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5022, 0.5008, 0.4743, 0.4992, 0.5079, 0.5236, 0.4812, 0.5120, 0.4925,\n",
      "         0.5125, 0.4995, 0.5170, 0.4876, 0.5154, 0.4789, 0.5178, 0.4996, 0.4907],\n",
      "        [0.5072, 0.5095, 0.4765, 0.5007, 0.4962, 0.5243, 0.4927, 0.5305, 0.4852,\n",
      "         0.5164, 0.5002, 0.5076, 0.4837, 0.5136, 0.4980, 0.5163, 0.5067, 0.4980],\n",
      "        [0.5040, 0.5053, 0.4681, 0.4996, 0.5077, 0.5324, 0.4773, 0.5110, 0.4914,\n",
      "         0.5193, 0.5004, 0.5141, 0.4884, 0.5216, 0.4898, 0.5180, 0.5076, 0.4855],\n",
      "        [0.5069, 0.5018, 0.4684, 0.4967, 0.5028, 0.5267, 0.4879, 0.5079, 0.4938,\n",
      "         0.5187, 0.5076, 0.5182, 0.4922, 0.5175, 0.4976, 0.5291, 0.5101, 0.5003]],\n",
      "       grad_fn=<SigmoidBackward>) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.4898, 0.4880, 0.4649, 0.4772, 0.4863, 0.4912, 0.4868, 0.4805, 0.4939,\n",
      "         0.5195, 0.4991, 0.5215, 0.4839, 0.4933, 0.4668, 0.5078, 0.4904, 0.4858],\n",
      "        [0.4690, 0.4999, 0.4723, 0.4840, 0.4881, 0.5073, 0.4935, 0.4834, 0.4927,\n",
      "         0.5265, 0.4961, 0.5171, 0.4906, 0.4806, 0.4636, 0.5021, 0.4851, 0.4869],\n",
      "        [0.4796, 0.4901, 0.4651, 0.4773, 0.4998, 0.5067, 0.4746, 0.4791, 0.4916,\n",
      "         0.5222, 0.4930, 0.5204, 0.4841, 0.4875, 0.4769, 0.5089, 0.4802, 0.4873],\n",
      "        [0.4812, 0.4894, 0.4695, 0.4820, 0.4960, 0.4920, 0.4892, 0.4926, 0.4879,\n",
      "         0.5205, 0.4983, 0.5147, 0.4896, 0.4993, 0.4637, 0.5015, 0.4814, 0.4920]],\n",
      "       grad_fn=<SigmoidBackward>) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0.4603, 0.4813, 0.4560, 0.4569, 0.4841, 0.4591, 0.4907, 0.4569, 0.4859,\n",
      "         0.5280, 0.4851, 0.5147, 0.4959, 0.4656, 0.4592, 0.4683, 0.4577, 0.4903],\n",
      "        [0.4608, 0.4641, 0.4571, 0.4471, 0.4880, 0.4497, 0.4979, 0.4532, 0.4908,\n",
      "         0.5230, 0.4941, 0.5154, 0.4823, 0.4556, 0.4515, 0.4836, 0.4722, 0.4850],\n",
      "        [0.4629, 0.4648, 0.4645, 0.4575, 0.4832, 0.4645, 0.4880, 0.4479, 0.4881,\n",
      "         0.5155, 0.5025, 0.5223, 0.4959, 0.4509, 0.4597, 0.4825, 0.4577, 0.4872],\n",
      "        [0.4687, 0.4746, 0.4526, 0.4714, 0.4884, 0.4576, 0.4956, 0.4604, 0.4877,\n",
      "         0.5191, 0.4915, 0.5139, 0.4943, 0.4645, 0.4640, 0.4914, 0.4725, 0.4941]],\n",
      "       grad_fn=<SigmoidBackward>) tensor([[1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-56af20cbd0ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_combiner_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-16933abe2afc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epoch)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# lossから、gradientを逆伝播させ、パラメータをupdateする。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# validation用のロースを計算する。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-16933abe2afc>\u001b[0m in \u001b[0;36m_update_params\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# ロスから、gradientを逆伝播し、パラメータをアップデートする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_combiner_handler.train(n_epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つづきはGoogleColaboratoryで行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
